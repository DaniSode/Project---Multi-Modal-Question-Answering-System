{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca4a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from model import VQAModel\n",
    "from build_dataset import VQADataset\n",
    "\n",
    "device = torch.device('cuda')\n",
    "data_dir = '../data'\n",
    "ckpt_dir = '../ckpt/best_model.pth'\n",
    "res_dir = '../Results'\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "FEATURE_SIZE, WORD_EMBED = 1024, 300\n",
    "MAX_QU_LEN, NUM_HIDDEN, HIDDEN_SIZE = 30, 2, 512\n",
    "\n",
    "def test(input_dir, data_type, batch_size, num_worker):\n",
    "\n",
    "    \"\"\"\n",
    "    results = [result]\n",
    "    result{ \"question_id\": int,\n",
    "            \"answer\": str }......\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # convert to (C,H,W) and [0,1]\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # mean=0; std=1\n",
    "    ])\n",
    "    vqa_dataset = VQADataset(input_dir, f'{data_type}.npy', max_qu_len=MAX_QU_LEN, transform= transform)\n",
    "    dataloader = DataLoader(vqa_dataset, batch_size=batch_size, shuffle=False, num_workers=num_worker)\n",
    "\n",
    "    qu_vocab_size = vqa_dataset.qu_vocab.vocab_size\n",
    "    ans_vocab_size = vqa_dataset.ans_vocab.vocab_size\n",
    "\n",
    "    model = VQAModel(feature_size=FEATURE_SIZE, qu_vocab_size=qu_vocab_size, ans_vocab_size=ans_vocab_size,\n",
    "                     word_embed=WORD_EMBED, hidden_size=HIDDEN_SIZE, num_hidden=NUM_HIDDEN).to(device)\n",
    "    model.load_state_dict(torch.load(ckpt_dir))\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    for idx, sample in enumerate(dataloader):\n",
    "\n",
    "        image = sample['image'].to(device)\n",
    "        question = sample['question'].to(device)\n",
    "        question_id = sample['question_id'].tolist()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(image, question)\n",
    "            predict = torch.log_softmax(logits, dim=1)\n",
    "\n",
    "        predict = torch.argmax(predict, dim=1).tolist()\n",
    "        predict = [vqa_dataset.ans_vocab.idx2word(idx) for idx in predict]\n",
    "        ans_qu_pair = [{'answer': ans, 'question_id': id} for ans, id in zip(predict, question_id)]\n",
    "        results.extend(ans_qu_pair)\n",
    "        if (idx+1) % 50 == 0:\n",
    "            print(f'finishing {data_type} set : {(idx+1)*batch_size} / {len(vqa_dataset)}')\n",
    "\n",
    "    if not os.path.exists(res_dir): os.makedirs(res_dir)\n",
    "    with open(os.path.join(res_dir, f'v2_OpenEnded_mscoco_{data_type}2014_results.json'), 'w') as f:\n",
    "        f.write(json.dumps(results))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    test(data_dir, 'val', batch_size=BATCH_SIZE, num_worker=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
