{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0d7e9d-8279-471a-bf01-0eeb756fd454",
   "metadata": {},
   "source": [
    "## Reduce the size of the dataset and preprocess the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd95ba-b54a-4c76-b324-52cfc64ff750",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8d4cd63-85f8-4e03-806b-93dc91209a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\soder\\anaconda3\\envs\\dmlproject\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resizing img\n",
    "import os\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Make vocab\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Preprocess data\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "# Build dataset\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Training\n",
    "import time\n",
    "from torch import optim\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ded4b8-1c2d-4772-b436-47b207c0d210",
   "metadata": {},
   "source": [
    "Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd49c8ee-593b-4e49-9bd4-d1a154fd3faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pth = 'dataset\\img'\n",
    "ann_pth = 'dataset\\\\ann'\n",
    "qst_pth = 'dataset\\qst'\n",
    "out_img_pth = 'preprocessed\\img'\n",
    "out_data_pth = 'preprocessed\\data'\n",
    "out_vocab_pth = 'preprocessed\\\\vocab'\n",
    "out_ann_pth = 'preprocessed\\\\ann'\n",
    "out_qst_pth = 'preprocessed\\qst'\n",
    "ckpt_pth = 'late_fusion\\ckpt'\n",
    "log_pth = 'late_fusion\\log'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea4bd28-1e1d-49e0-8921-2bc1859fe530",
   "metadata": {},
   "source": [
    "Preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f25a66e4-f269-4fa1-b693-1d9bea0859b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500/4000] resized images and saved into 'preprocessed\\img\\img_test'.\n",
      "[1000/4000] resized images and saved into 'preprocessed\\img\\img_test'.\n",
      "[1500/4000] resized images and saved into 'preprocessed\\img\\img_test'.\n",
      "[2000/4000] resized images and saved into 'preprocessed\\img\\img_test'.\n",
      "[2500/4000] resized images and saved into 'preprocessed\\img\\img_test'.\n",
      "[3000/4000] resized images and saved into 'preprocessed\\img\\img_test'.\n",
      "[3500/4000] resized images and saved into 'preprocessed\\img\\img_test'.\n",
      "[4000/4000] resized images and saved into 'preprocessed\\img\\img_test'.\n",
      "[500/4000] resized images and saved into 'preprocessed\\img\\img_train'.\n",
      "[1000/4000] resized images and saved into 'preprocessed\\img\\img_train'.\n",
      "[1500/4000] resized images and saved into 'preprocessed\\img\\img_train'.\n",
      "[2000/4000] resized images and saved into 'preprocessed\\img\\img_train'.\n",
      "[2500/4000] resized images and saved into 'preprocessed\\img\\img_train'.\n",
      "[3000/4000] resized images and saved into 'preprocessed\\img\\img_train'.\n",
      "[3500/4000] resized images and saved into 'preprocessed\\img\\img_train'.\n",
      "[4000/4000] resized images and saved into 'preprocessed\\img\\img_train'.\n",
      "[500/2000] resized images and saved into 'preprocessed\\img\\img_val'.\n",
      "[1000/2000] resized images and saved into 'preprocessed\\img\\img_val'.\n",
      "[1500/2000] resized images and saved into 'preprocessed\\img\\img_val'.\n",
      "[2000/2000] resized images and saved into 'preprocessed\\img\\img_val'.\n"
     ]
    }
   ],
   "source": [
    "def resize_image(image, size):\n",
    "    \"\"\"Resize an image to the given size.\"\"\"\n",
    "    return image.resize(size, Image.Resampling.LANCZOS)\n",
    "\n",
    "def resize_images(input_dir, output_dir, size, split_ratio):\n",
    "    \"\"\"Resize the images in 'input_dir' and save into 'output_dir'.\"\"\"\n",
    "    for idir in os.scandir(input_dir):\n",
    "        if not idir.is_dir():\n",
    "            print('No valid directory')\n",
    "            continue\n",
    "        if not os.path.exists(output_dir+'\\\\'+idir.name):\n",
    "            os.makedirs(output_dir+'\\\\'+idir.name)\n",
    "        else:\n",
    "            for file in os.listdir(output_dir+'\\\\'+idir.name):\n",
    "                if os.path.isfile(os.path.join(output_dir+'\\\\'+idir.name, file)):\n",
    "                    os.remove(os.path.join(output_dir+'\\\\'+idir.name, file))\n",
    "                      \n",
    "        images = os.listdir(idir.path)\n",
    "        images, _ = train_test_split(images,\n",
    "                                     test_size=split_ratio,\n",
    "                                     shuffle=False)\n",
    "        n_images = len(images)\n",
    "        for id, image in enumerate(images):\n",
    "            try:\n",
    "                with open(os.path.join(idir.path, image), 'r+b') as f:\n",
    "                    with Image.open(f) as img:\n",
    "                        img = resize_image(img, size)\n",
    "                        img.save(os.path.join(output_dir+'\\\\'+idir.name, image), img.format)\n",
    "            except(IOError, SyntaxError) as e:\n",
    "                pass\n",
    "            if (id+1) % 500 == 0:\n",
    "                print(\"[{}/{}] resized images and saved into '{}'.\"\n",
    "                      .format(id+1, n_images, output_dir+'\\\\'+idir.name))\n",
    "                    \n",
    "def main():\n",
    "\n",
    "    input_dir = img_pth\n",
    "    output_dir = out_img_pth\n",
    "    image_size = [224, 224]\n",
    "    split_ratio = 0.8\n",
    "    resize_images(input_dir, output_dir, image_size, split_ratio)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa942d18-ad10-4d56-a8a3-f8123f678ead",
   "metadata": {},
   "source": [
    "Removing questions and annotations that doesnt belong to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67a8c875-fa8b-4b94-a19e-7fd9ea4f2071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 remaining annotations of 60000 in ann_train.json\n",
      "6000 remaining annotations of 30000 in ann_val.json\n",
      "12000 remaining questions of 60000 in multi_qst_test.json\n",
      "12000 remaining questions of 60000 in open_qst_test.json\n",
      "12000 remaining questions of 60000 in multi_qst_train.json\n",
      "12000 remaining questions of 60000 in open_qst_train.json\n",
      "6000 remaining questions of 30000 in multi_qst_val.json\n",
      "6000 remaining questions of 30000 in open_qst_val.json\n"
     ]
    }
   ],
   "source": [
    "def removing(out_img_pth, ann_pth, qst_pth):\n",
    "\n",
    "    test_ids = []\n",
    "    train_ids = []\n",
    "    val_ids = []\n",
    "    for idir in os.scandir(out_img_pth):\n",
    "        for file in os.listdir(idir):\n",
    "            components = file.split('_')\n",
    "            image_id = components[-1].split('.')[0]\n",
    "            numeric_part = int(image_id)\n",
    "            if 'test' in idir.name:\n",
    "                test_ids.append(numeric_part)\n",
    "            elif 'train' in idir.name:\n",
    "                train_ids.append(numeric_part)\n",
    "            elif 'val' in idir.name:\n",
    "                val_ids.append(numeric_part)\n",
    "    \n",
    "    for idir in os.scandir(ann_pth):\n",
    "        if 'test' in idir.name:\n",
    "            ids = test_ids\n",
    "        elif 'train' in idir.name:\n",
    "            ids = train_ids\n",
    "        elif 'val' in idir.name:\n",
    "            ids = val_ids\n",
    "        \n",
    "        for file in os.listdir(idir):\n",
    "            path = os.path.join(idir, file)\n",
    "            with open(path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            annotations = data['annotations']\n",
    "            prelen = len(annotations)\n",
    "            labels = dict()\n",
    "            number = 0\n",
    "            for label in annotations:\n",
    "                    if int(label['image_id']) in ids:\n",
    "                        labels.update({number: label})   \n",
    "                        number += 1\n",
    "            print(f'{len(labels)} remaining annotations of {prelen} in {file}')\n",
    "            data['annotations'] = labels\n",
    "            with open(out_ann_pth + '\\\\' + file, 'w') as f:\n",
    "                json.dump(data, f)\n",
    "    \n",
    "    for idir in os.scandir(qst_pth):\n",
    "        if 'test' in idir.name:\n",
    "            ids = test_ids\n",
    "        elif 'train' in idir.name:\n",
    "            ids = train_ids\n",
    "        elif 'val' in idir.name:\n",
    "            ids = val_ids\n",
    "            \n",
    "        for file in os.listdir(idir):\n",
    "            path = os.path.join(idir, file)\n",
    "            with open(path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            questions = data['questions']\n",
    "            prelen = len(questions)\n",
    "            labels = dict()\n",
    "            number = 0\n",
    "            for label in questions:\n",
    "                    if int(label['image_id']) in ids:\n",
    "                        labels.update({number: label}) \n",
    "                        number += 1\n",
    "            print(f'{len(labels)} remaining questions of {prelen} in {file}')\n",
    "            data['questions'] = labels\n",
    "            with open(out_qst_pth + '\\\\' + file, 'w') as f:\n",
    "                json.dump(data, f)\n",
    "\n",
    "def main():\n",
    "    img_pth = out_img_pth\n",
    "    annot_pth = ann_pth\n",
    "    quest_pth = qst_pth\n",
    "    removing(img_pth, annot_pth, quest_pth)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c642598-fedf-4090-ad04-fa25e44db3e4",
   "metadata": {},
   "source": [
    "Make vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfd36008-1d55-43e3-8869-02eb844e18dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of total words of questions in multi_qst_train.json: 2371\n",
      "The number of total words of questions in multi_qst_val.json: 1788\n",
      "The number of total words of questions in open_qst_train.json: 2371\n",
      "The number of total words of questions in open_qst_val.json: 1788\n",
      "The number of total words of answers in ann_train.json: 5634\n",
      "The number of total words of answers in ann_val.json: 3490\n",
      "The number of total words of answers in ann_vocabs will be: 7606\n",
      "The number of total words of answers in qst_vocabs will be: 2838\n"
     ]
    }
   ],
   "source": [
    "def make_q_vocab(input_dir, output_dir):\n",
    "\n",
    "    for file in os.scandir(input_dir):\n",
    "        if \"test\" in file.name:\n",
    "            continue\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir) \n",
    "            \n",
    "        regex = re.compile(r'(\\W+)')\n",
    "        q_vocab = []\n",
    "        path = os.path.join(input_dir, file.name)\n",
    "        with open(path, 'r') as f:\n",
    "            q_data = json.load(f)\n",
    "        question = q_data['questions'].values()\n",
    "        for quest in question:\n",
    "            split = regex.split(quest['question'].lower())\n",
    "            tmp = [w.strip() for w in split if len(w.strip()) > 0]\n",
    "            q_vocab.extend(tmp)\n",
    "    \n",
    "        q_vocab = list(set(q_vocab))\n",
    "        q_vocab.sort()\n",
    "        q_vocab.insert(0, '<pad>')\n",
    "        q_vocab.insert(1, '<unk>')\n",
    "    \n",
    "        with open(output_dir + '\\\\' + file.name.split(\".\")[0] + '_vocabs.txt', 'w') as f:\n",
    "            f.writelines([v+'\\n' for v in q_vocab])\n",
    "\n",
    "        print(f'The number of total words of questions in {file.name}: {len(q_vocab)}')\n",
    "\n",
    "def make_a_vocab(input_dir, output_dir):\n",
    "\n",
    "    for file in os.scandir(input_dir):\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir) \n",
    "            \n",
    "        answers = defaultdict(lambda :0)\n",
    "        path = os.path.join(input_dir, file.name)\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        annotations = data['annotations'].values()\n",
    "        for label in annotations:\n",
    "            for ans in label['answers']:\n",
    "                vocab = ans['answer']\n",
    "                if re.search(r'[^\\w\\s]', vocab):\n",
    "                    continue\n",
    "                answers[vocab] += 1\n",
    "    \n",
    "        answers = sorted(answers, key=answers.get, reverse= True) \n",
    "        with open(output_dir + '\\\\' + file.name.split(\".\")[0] + '_vocabs.txt', 'w') as f :\n",
    "            f.writelines([ans+'\\n' for ans in answers])\n",
    "            \n",
    "        print(f'The number of total words of answers in {file.name}: {len(answers)}')\n",
    "\n",
    "def make_vocab(output_dir):\n",
    "    ann_vocab = set()\n",
    "    qst_vocab = set()\n",
    "    for file in os.scandir(output_dir):\n",
    "        if 'ann' in file.name:\n",
    "            with open(file.path, 'r') as f:\n",
    "                for line in f: \n",
    "                    ann_vocab.add(line.split('\\n')[0])\n",
    "        elif 'qst' in file.name:\n",
    "            with open(file.path, 'r') as f:\n",
    "                for line in f: \n",
    "                    qst_vocab.add(line.split('\\n')[0])\n",
    "\n",
    "    print(f'The number of total words of answers in ann_vocabs will be: {len(ann_vocab)}')\n",
    "    print(f'The number of total words of answers in qst_vocabs will be: {len(qst_vocab)}')\n",
    "    \n",
    "    with open(output_dir + '\\\\ann_vocabs.txt', 'w') as f:\n",
    "        f.writelines([ans+'\\n' for ans in ann_vocab])\n",
    "    with open(output_dir + '\\\\qst_vocabs.txt', 'w') as f:\n",
    "        f.writelines([ans+'\\n' for ans in qst_vocab])\n",
    "\n",
    "def main():\n",
    "    input_qst_dir = out_qst_pth\n",
    "    input_ann_dir = out_ann_pth\n",
    "    output_vocab_dir = out_vocab_pth\n",
    "    make_q_vocab(input_qst_dir, output_vocab_dir)\n",
    "    make_a_vocab(input_ann_dir, output_vocab_dir)\n",
    "    make_vocab(out_vocab_pth)\n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30a95f8-248b-4314-8e61-b1ec61c50266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
